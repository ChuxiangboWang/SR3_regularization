{
    "name": "tv0_pretrain",
    "phase": "pretrain", // train | val | pretrain
    
    "pretrain": {
        "script": "pretrain_CNN/train.py",  
        "lr": 16,
        "hr": 128,
        "hr_dir": "./dataset/celebahq_16_128/hr_128",
        "lr_dir": "./dataset/celebahq_16_128/lr_16",  
        "batch_size": 16,
        "batch_size_eval": 1,  // Increase with large data set
        "learning_rate": 1.0,
        "epochs": 2000, 
        "num_workers": 4,
        "extra_args": ["-c", "config/celebahq.json"],
        "ckpt_path": null,
        "TVrelu": 1
      },
    
    "gpu_ids": [0],
    "path": { //set the path
        "log": "logs",
        "tb_logger": "tb_logger",
        "results": "results",
        "checkpoint": "checkpoint",
        "resume_state": null
        // "experiments/celeb_tv0_leakystdrelu_250825_150651/checkpoint/I500000_E67"
    },
    "datasets": {
        "train": {
            "name": "celebahq",
            "mode": "HR", // whether need LR img
            "dataroot": "dataset/celebahq_64_128",//output
            "datatype": "img", //lmdb or img, path of img files
            "l_resolution": 64, // low resolution need to super_resolution
            "r_resolution": 128, // high resolution
            "batch_size": 4,
            "num_workers": 4,
            "use_shuffle": true,
            "data_len": -1, // -1 represents all data used in train
            "cnn_sr": true // whether use CNN processed LR
        },
        "val": {
            "name": "celebahq",
            "mode": "LRHR",
            "dataroot": "dataset/celebahq_64_128",
            "datatype": "img", //lmdb or img, path of img files
            "l_resolution": 64,
            "r_resolution": 128,
            "data_len": 3, // data length in validation 
            "n_run":200,
            "cnn_sr": true // whether use CNN processed LR as input
        
        }
    },
    "model": {
        "which_model_G": "sr3", // use the ddpm or sr3 network structure
        "finetune_norm": false,
        "unet": {
            "in_channel": 6,
            "out_channel": 3,
            "inner_channel": 64,
            "channel_multiplier": [
                1,
                2,
                4,
                8,
                8
            ],
            "attn_res": [
                16
            ],
            "res_blocks": 2,
            "dropout": 0.2,
            "final_activation": "swish", // default swish. Can be chosen from, swish,relu,and leakyrelu
            "leaky_alpha": 0.2 // alpha for the leaky relu
        },
        "beta_schedule": { // use munual beta_schedule for acceleration
            "train": {
                "schedule": "linear",
                "n_timestep": 2000,
                "linear_start": 1e-6,
                "linear_end": 1e-2
            },
            "val": {
                "schedule": "linear",
                "n_timestep": 2000,
                "linear_start": 1e-6,
                "linear_end": 1e-2
            }
        },
        "diffusion": {
            "image_size": 128,
            "channels": 3, //sample channel
            "conditional": true // unconditional generation or unconditional generation(super_resolution)
        },
        "loss": {
            "type": "l1", // l1 or l2
            "TV1_weight": 1000.0,
            "TV2_weight": 0.0,
            "TVF_weight": 0.0,
            "TVF_alpha": 0.0,//default is 1.6
            "wavelet_l1_weight": 0.0, //10000.0,
            "wavelet_type": "haar" //default is "haar". Other options includes "db", "sym", "coif"  (https://pywavelets.readthedocs.io/en/latest/ref/wavelets.html)
        }
    },
    "train": {
        "n_iter": 1000000,
        "val_freq": 1e04,
        "save_checkpoint_freq": 1e4,
        "print_freq": 200,
        "optimizer": {
            "type": "adam",
            "lr": 1e-4
        },
        "ema_scheduler": { // not used now
            "step_start_ema": 5000,
            "update_ema_every": 1,
            "ema_decay": 0.9999
        }
    },
    "wandb": {
        "project": "sr_ffhq"
    }
}
